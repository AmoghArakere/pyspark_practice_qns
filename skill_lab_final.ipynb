{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0XA8+frLyYTYLkqNEGr2B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmoghArakere/pyspark_practice_qns/blob/main/skill_lab_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "   \n",
        "# **HR_Analytics**\n",
        "\n"
      ],
      "metadata": {
        "id": "LuOoBcAsBDAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when, mean, udf, expr\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "spark = SparkSession.builder.appName(\"HRDatasetCleaning\").getOrCreate()\n",
        "\n",
        "data = spark.read.csv(\"HR_Analytics.csv\", header=True, inferSchema=True)\n",
        "\n",
        "required_columns = [col_name for col_name in data.columns if not col_name.startswith(\"_c\")]\n",
        "data = data.select(*required_columns)\n",
        "\n",
        "data = data.withColumn(\"BusinessTravel\", when(col(\"BusinessTravel\").isNull(), \"Unknown\").otherwise(col(\"BusinessTravel\")))\n",
        "\n",
        "mean_distance = data.select(mean(col(\"DistanceFromHome\"))).collect()[0][0]\n",
        "data = data.withColumn(\"DistanceFromHome\", when(col(\"DistanceFromHome\").isNull(), mean_distance).otherwise(col(\"DistanceFromHome\")))\n",
        "\n",
        "data = data.withColumn(\"EducationField\", when(col(\"EducationField\").isNull(), \"Other\").otherwise(col(\"EducationField\")))\n",
        "\n",
        "data = data.dropDuplicates([\"EmpID\"])\n",
        "\n",
        "data = data.filter(col(\"DistanceFromHome\") <= 50)\n",
        "\n",
        "data = data.withColumn(\n",
        "    \"PercentSalaryHike_Category\",\n",
        "    when(col(\"PercentSalaryHike\") <= 10, \"Low\")\n",
        "    .when((col(\"PercentSalaryHike\") > 10) & (col(\"PercentSalaryHike\") <= 20), \"Moderate\")\n",
        "    .otherwise(\"High\")\n",
        ")\n",
        "\n",
        "job_sat_map = {1: \"Poor\", 2: \"Below Average\", 3: \"Average\", 4: \"Excellent\"}\n",
        "job_sat_udf = udf(lambda x: job_sat_map.get(x), StringType())\n",
        "data = data.withColumn(\"JobSatisfaction\", job_sat_udf(col(\"JobSatisfaction\")))\n",
        "\n",
        "data = data.withColumn(\"SalarySlab\", expr(\"CAST(SUBSTRING(SalarySlab, 1, LENGTH(SalarySlab) - 1) AS INT) * 1000\"))\n",
        "\n",
        "data.show()\n",
        "data.write.csv(\"cleaned_hr_analytics12.csv\", header=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SO19YuGOe5G2",
        "outputId": "53bad2d3-01d4-4758-aeee-05170aee1bb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+--------+-----------------+--------------------+----------------+---------+--------------+------+--------------------+---------------+-------------+----------+--------+-----------------+--------------------------+\n",
            "|EmpID|Age|AgeGroup|   BusinessTravel|          Department|DistanceFromHome|Education|EducationField|Gender|             JobRole|JobSatisfaction|MaritalStatus|SalarySlab|OverTime|PercentSalaryHike|PercentSalaryHike_Category|\n",
            "+-----+---+--------+-----------------+--------------------+----------------+---------+--------------+------+--------------------+---------------+-------------+----------+--------+-----------------+--------------------------+\n",
            "|RM004| 33|   26-35|Travel_Frequently|Research & Develo...|             3.0|        4| Life Sciences|Female|  Research Scientist|        Average|      Married|     19000|     Yes|               11|                  Moderate|\n",
            "|RM005| 27|   26-35|    Travel_Rarely|Research & Develo...|             2.0|        1|       Medical|  Male|Laboratory Techni...|  Below Average|      Married|     20000|      No|               12|                  Moderate|\n",
            "|RM006| 32|   26-35|Travel_Frequently|Research & Develo...|             2.0|        2| Life Sciences|  Male|Laboratory Techni...|      Excellent|       Single|     19000|      No|               13|                  Moderate|\n",
            "|RM008| 30|   26-35|    Travel_Rarely|Research & Develo...|            24.0|        1| Life Sciences|  Male|Laboratory Techni...|        Average|     Divorced|     19000|      No|               22|                      High|\n",
            "|RM011| 35|   26-35|    Travel_Rarely|Research & Develo...|            16.0|        3|       Medical|  Male|Laboratory Techni...|  Below Average|      Married|     19000|      No|               13|                  Moderate|\n",
            "|RM012| 29|   26-35|    Travel_Rarely|Research & Develo...|            15.0|        2| Life Sciences|Female|Laboratory Techni...|        Average|       Single|     15000|     Yes|               12|                  Moderate|\n",
            "|RM013| 31|   26-35|    Travel_Rarely|Research & Develo...|            26.0|        1| Life Sciences|  Male|  Research Scientist|        Average|     Divorced|     19000|      No|               17|                  Moderate|\n",
            "|RM014| 34|   26-35|     TravelRarely|Research & Develo...|            19.0|        2|       Medical|  Male|Laboratory Techni...|      Excellent|     Divorced|     19000|      No|               11|                  Moderate|\n",
            "|RM015| 28|   26-35|    Travel_Rarely|Research & Develo...|            24.0|        3| Life Sciences|  Male|Laboratory Techni...|        Average|       Single|     20000|     Yes|               14|                  Moderate|\n",
            "|RM016| 29|   26-35|    Travel_Rarely|Research & Develo...|            21.0|        4| Life Sciences|Female|Manufacturing Dir...|           Poor|     Divorced|     15000|      No|               11|                  Moderate|\n",
            "|RM017| 32|   26-35|    Travel_Rarely|Research & Develo...|             5.0|        2| Life Sciences|  Male|  Research Scientist|  Below Average|     Divorced|     19000|     Yes|               12|                  Moderate|\n",
            "|RM018| 22|   18-25|       Non-Travel|Research & Develo...|            16.0|        2|       Medical|  Male|Laboratory Techni...|      Excellent|     Divorced|      5000|     Yes|               13|                  Moderate|\n",
            "|RM021| 24|   18-25|       Non-Travel|Research & Develo...|            11.0|        2|         Other|Female|Manufacturing Dir...|        Average|     Divorced|      5000|      No|               18|                  Moderate|\n",
            "|RM023| 34|   26-35|     TravelRarely|Research & Develo...|             7.0|        4| Life Sciences|Female|   Research Director|  Below Average|       Single|     19000|      No|               11|                  Moderate|\n",
            "|RM025| 34|   26-35|    Travel_Rarely|Research & Develo...|             6.0|        1|       Medical|  Male|  Research Scientist|           Poor|       Single|     19000|      No|               11|                  Moderate|\n",
            "|RM027| 32|   26-35|Travel_Frequently|Research & Develo...|            16.0|        1| Life Sciences|Female|  Research Scientist|           Poor|       Single|     19000|     Yes|               22|                      High|\n",
            "|RM031| 33|   26-35|    Travel_Rarely|Research & Develo...|             2.0|        3|       Medical|  Male|Laboratory Techni...|      Excellent|       Single|     19000|      No|               11|                  Moderate|\n",
            "|RM033| 30|   26-35|    Travel_Rarely|Research & Develo...|             9.0|        2|       Medical|  Male|Laboratory Techni...|        Average|       Single|     19000|      No|               13|                  Moderate|\n",
            "|RM035| 24|   18-25|    Travel_Rarely|Research & Develo...|             1.0|        3|       Medical|  Male|  Research Scientist|      Excellent|      Married|      5000|     Yes|               16|                  Moderate|\n",
            "|RM038| 35|   26-35|    Travel_Rarely|               Sales|             2.0|        3|     Marketing|Female|Sales Representative|      Excellent|      Married|     19000|      No|               13|                  Moderate|\n",
            "+-----+---+--------+-----------------+--------------------+----------------+---------+--------------+------+--------------------+---------------+-------------+----------+--------+-----------------+--------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "#**Real estate**\n"
      ],
      "metadata": {
        "id": "B8mT2NruBWdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "spark = SparkSession.builder.appName(\"RealEstateDataCleaning\").getOrCreate()\n",
        "data = spark.read.csv(\"Real_estate.csv\", header=True, inferSchema=True)\n",
        "data=data.dropna()\n",
        "data = data.withColumn(\"Date Recorded\", to_date(col(\"Date Recorded\"), \"MM/dd/yyyy\"))\n",
        "data=data.dropna()\n",
        "data = data.withColumn(\"Sales Ratio\", col(\"Assessed Value\") / col(\"Sale Amount\"))\n",
        "data = data.withColumn(\"Residential Type\", when(col(\"Residential Type\").isNull(), \"Unknown\").otherwise(col(\"Residential Type\")))\n",
        "data = data.filter((col(\"List Year\") >= 2001) & (col(\"List Year\") <= 2022))\n",
        "mode_town = data.groupBy(\"Town\").count().orderBy(\"count\", ascending=False).first()[0]\n",
        "data = data.withColumn(\"Town\", when(col(\"Town\").isNull(), lit(mode_town)).otherwise(col(\"Town\")))\n",
        "data.show()\n",
        "data.write.csv(\"cleaned_real_estate21.csv\", header=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jofedlXYzO2A",
        "outputId": "c7fe3efd-9198-4965-c128-9204f312a97b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+---------+-------------+------------+--------------------+--------------+-----------+-------------+----------------+-------------------+\n",
            "|Serial Number|List Year|Date Recorded|        Town|             Address|Assessed Value|Sale Amount|Property Type|Residential Type|        Sales Ratio|\n",
            "+-------------+---------+-------------+------------+--------------------+--------------+-----------+-------------+----------------+-------------------+\n",
            "|       220008|     2022|   2023-01-30|     Andover|         618 ROUTE 6|        139020|     232000|  Residential|   Single Family| 0.5992241379310345|\n",
            "|       200243|     2020|   2021-04-13|        Avon|111 NORTHINGTON D...|        619290|     890000|  Residential|   Single Family| 0.6958314606741574|\n",
            "|        22043|     2022|   2023-03-15|Beacon Falls|   41 EDGEWOOD DRIVE|        164170|     285000|  Residential|   Single Family| 0.5760350877192982|\n",
            "|       220440|     2022|   2023-02-16|    Branford|          69 MONTOYA|         84300|     195000|  Residential|           Condo| 0.4323076923076923|\n",
            "|       211762|     2021|   2022-06-21|  Bridgeport|     38 RIVERVIEW DR|         86990|     247000|  Residential|           Condo|0.35218623481781375|\n",
            "|       200354|     2020|   2020-12-29|     Bristol|      391 TIFFANY LA|        173740|     299000|  Residential|   Single Family| 0.5810702341137124|\n",
            "|     22000107|     2022|   2022-12-22|    Cheshire|       30 CURRIER PL|        129150|     265000|  Residential|           Condo|0.48735849056603775|\n",
            "|       201212|     2020|   2021-08-23|     Bristol|     8 JENNINGS TERR|         88060|      91000|  Residential|   Single Family| 0.9676923076923077|\n",
            "|     20000179|     2020|   2021-02-22|  Brookfield|     72 HOMESTEAD LN|         94770|     170000|  Residential|           Condo| 0.5574705882352942|\n",
            "|       200042|     2020|   2021-05-19|      Canaan|          93 MAIN ST|        355800|     599000|  Residential|      Two Family| 0.5939899833055092|\n",
            "|      2000230|     2020|   2021-03-16|    Cheshire|         50 ABBEY CT|        210940|     365000|  Residential|   Single Family|  0.577917808219178|\n",
            "|       220054|     2022|   2022-12-13|    Coventry|  166 CEDAR SWAMP RD|        114500|     255000|  Residential|   Single Family|0.44901960784313727|\n",
            "|      2000330|     2020|   2021-05-20|    Cheshire|    844 HIGHLAND AVE|        144220|     204000|  Residential|   Single Family| 0.7069607843137254|\n",
            "|       200078|     2020|   2021-09-20|     Chester|        6 SUNSET AVE|        250100|     418800|  Residential|   Single Family| 0.5971824259789876|\n",
            "|       200035|     2020|   2020-10-29|  Colchester|     160 SHADBUSH DR|        223900|     392000|  Residential|   Single Family| 0.5711734693877552|\n",
            "|       200742|     2020|   2021-03-29|     Danbury|    17 OHEHYAHTAH PL|        339100|     550000|  Residential|   Single Family| 0.6165454545454545|\n",
            "|       200938|     2020|   2021-05-13|     Danbury|15-17 BOUGHTON ST...|        118900|     230000|  Residential|           Condo| 0.5169565217391304|\n",
            "|       220261|     2022|   2023-07-13|   East Lyme|          2 PEACH LN|        465500|     827000|  Residential|   Single Family| 0.5628778718258767|\n",
            "|       201580|     2020|   2021-09-27|     Danbury|         25 CROWN ST|        192300|     525000|  Residential|      Two Family|0.36628571428571427|\n",
            "|       200408|     2020|   2020-12-24|     Danbury|   15 SCUPPO RD 1503|        130800|     260000|  Residential|           Condo| 0.5030769230769231|\n",
            "+-------------+---------+-------------+------------+--------------------+--------------+-----------+-------------+----------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# **Student Info**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "d1kHnqpJ4F4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "spark = SparkSession.builder.appName(\"StudentInfoCleaning\").getOrCreate()\n",
        "df = spark.read.csv(\"student-info.csv\", header=True, inferSchema=True)\n",
        "df = df.withColumn(\"school\",\n",
        "    when(col(\"school\") == \"GP\", \"Gabriel Pereira\")\n",
        "    .when(col(\"school\") == \"MS\", \"Mousinho da Silveira\")\n",
        "    .otherwise(col(\"school\"))\n",
        ")\n",
        "df = df.withColumn(\"address\",\n",
        "    when(col(\"address\") == \"U\", \"Urban\")\n",
        "    .when(col(\"address\") == \"R\", \"Rural\")\n",
        "    .otherwise(col(\"address\"))\n",
        ")\n",
        "df = df.withColumn(\"family_size\",\n",
        "    when(col(\"family_size\") == \"LE3\", \"Less or Equal to 3\")\n",
        "    .when(col(\"family_size\") == \"GT3\", \"Greater than 3\")\n",
        "    .otherwise(col(\"family_size\"))\n",
        ")\n",
        "def replace_education(column):\n",
        "    return when(column == 0, \"None\") \\\n",
        "        .when(column == 1, \"Primary Education (4th Grade)\") \\\n",
        "        .when(column == 2, \"5th to 9th Grade\") \\\n",
        "        .when(column == 3, \"Secondary Education\") \\\n",
        "        .when(column == 4, \"Higher Education\") \\\n",
        "        .otherwise(column)\n",
        "df = df.withColumn(\"mother_edu\", replace_education(col(\"mother_edu\"))) \\\n",
        "       .withColumn(\"father_edu\", replace_education(col(\"father_edu\")))\n",
        "df = df.fillna(\"other\", subset=[\"mother_job\"])\n",
        "df = df.withColumn(\"traveltime\",\n",
        "    when(col(\"traveltime\") == 1, \"15 min\")\n",
        "    .when(col(\"traveltime\") == 2, \"30 min\")\n",
        "    .when(col(\"traveltime\") == 3, \"45 min\")\n",
        "    .when(col(\"traveltime\") == 4, \"60 min\")\n",
        "    .otherwise(col(\"traveltime\"))\n",
        ")\n",
        "df = df.fillna(\"unknown\", subset=[\"internet\"])\n",
        "df = df.filter((col(\"age\") >= 15) & (col(\"age\") <= 22))\n",
        "df.show()\n",
        "df.printSchema()\n",
        "df.write.csv(\"cleaned_student_info1.csv\", header=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xNTlanR24MyN",
        "outputId": "498c041c-fb27-4c94-95e0-90c487438d0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+---+---+-------+------------------+--------------------+--------------------+----------+----------+----------+--------+\n",
            "|         school|sex|age|address|       family_size|          mother_edu|          father_edu|Mother_job|Father_job|traveltime|internet|\n",
            "+---------------+---+---+-------+------------------+--------------------+--------------------+----------+----------+----------+--------+\n",
            "|Gabriel Pereira|  F| 18|  Urban|    Greater than 3|    Higher Education|    Higher Education|   at_home|   teacher|    30 min|      no|\n",
            "|Gabriel Pereira|  F| 17|  Urban|    Greater than 3|Primary Education...|Primary Education...|   at_home|     other|    15 min|     yes|\n",
            "|Gabriel Pereira|  F| 15|  Urban|Less or Equal to 3|Primary Education...|Primary Education...|   at_home|     other|    15 min|     yes|\n",
            "|Gabriel Pereira|  F| 15|  Urban|    Greater than 3|    Higher Education|    5th to 9th Grade|    health|  services|    15 min|     yes|\n",
            "|Gabriel Pereira|  F| 16|  Urban|    Greater than 3| Secondary Education| Secondary Education|     other|     other|    15 min|      no|\n",
            "|Gabriel Pereira|  M| 16|  Urban|Less or Equal to 3|    Higher Education| Secondary Education|  services|     other|    15 min|     yes|\n",
            "|Gabriel Pereira|  M| 16|  Urban|Less or Equal to 3|    5th to 9th Grade|    5th to 9th Grade|     other|     other|    15 min|     yes|\n",
            "|Gabriel Pereira|  F| 17|  Urban|    Greater than 3|    Higher Education|    Higher Education|     other|   teacher|    30 min|      no|\n",
            "|Gabriel Pereira|  M| 15|  Urban|Less or Equal to 3| Secondary Education|    5th to 9th Grade|  services|     other|    15 min|     yes|\n",
            "|Gabriel Pereira|  M| 15|  Urban|    Greater than 3| Secondary Education|    Higher Education|     other|     other|    15 min|     yes|\n",
            "|Gabriel Pereira|  F| 15|  Urban|    Greater than 3|    Higher Education|    Higher Education|   teacher|    health|    15 min|     yes|\n",
            "|Gabriel Pereira|  F| 15|  Urban|    Greater than 3|    5th to 9th Grade|Primary Education...|  services|     other|    45 min|     yes|\n",
            "|Gabriel Pereira|  M| 15|  Urban|Less or Equal to 3|    Higher Education|    Higher Education|    health|  services|    15 min|     yes|\n",
            "|Gabriel Pereira|  M| 15|  Urban|    Greater than 3|    Higher Education| Secondary Education|   teacher|     other|    30 min|     yes|\n",
            "|Gabriel Pereira|  M| 15|  Urban|    Greater than 3|    5th to 9th Grade|    5th to 9th Grade|     other|     other|    15 min|     yes|\n",
            "|Gabriel Pereira|  F| 16|  Urban|    Greater than 3|    Higher Education|    Higher Education|    health|     other|    15 min| unknown|\n",
            "|Gabriel Pereira|  F| 16|  Urban|    Greater than 3|    Higher Education|    Higher Education|  services|  services|    15 min| unknown|\n",
            "|Gabriel Pereira|  F| 16|  Urban|    Greater than 3| Secondary Education| Secondary Education|     other|     other|    45 min| unknown|\n",
            "|Gabriel Pereira|  M| 17|  Urban|    Greater than 3| Secondary Education|    5th to 9th Grade|  services|  services|    15 min| unknown|\n",
            "|Gabriel Pereira|  M| 16|  Urban|Less or Equal to 3|    Higher Education| Secondary Education|    health|     other|    15 min| unknown|\n",
            "+---------------+---+---+-------+------------------+--------------------+--------------------+----------+----------+----------+--------+\n",
            "only showing top 20 rows\n",
            "\n",
            "root\n",
            " |-- school: string (nullable = true)\n",
            " |-- sex: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- address: string (nullable = true)\n",
            " |-- family_size: string (nullable = true)\n",
            " |-- mother_edu: string (nullable = true)\n",
            " |-- father_edu: string (nullable = true)\n",
            " |-- Mother_job: string (nullable = false)\n",
            " |-- Father_job: string (nullable = true)\n",
            " |-- traveltime: string (nullable = true)\n",
            " |-- internet: string (nullable = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# **Phone Usage**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FZ5oO8hT5SaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "spark = SparkSession.builder.appName(\"PhoneDatasetCleaning\").getOrCreate()\n",
        "data = spark.read.csv(\"phone_usage_india.csv\", header=True, inferSchema=True)\n",
        "data = data.filter((col(\"Age\") >= 18) & (col(\"Age\") <= 90))\n",
        "data = data.withColumn(\n",
        "    \"Age_Category\",\n",
        "    when(col(\"Age\") <= 35, \"Young\")\n",
        "    .when((col(\"Age\") > 35) & (col(\"Age\") <= 60), \"Middle-aged\")\n",
        "    .otherwise(\"Senior\")\n",
        ")\n",
        "data = data.withColumn(\"Location\", when(col(\"Location\").isNull(), \"Unknown\").otherwise(col(\"Location\")))\n",
        "time_columns = [\"Screen Time (hrs/day)\", \"Social Media Time (hrs/day)\", \"Streaming Time (hrs/day)\", \"Gaming Time (hrs/day)\"]\n",
        "for col_name in time_columns:\n",
        "    minutes_col = col_name.replace(\"hrs/day\", \"Minutes\")\n",
        "    data = data.withColumn(minutes_col, col(col_name) * 60)\n",
        "data = data.withColumn(\n",
        "    \"Screen Time (Minutes)\",\n",
        "    when(col(\"Screen Time (Minutes)\") <= 180, \"Low Usage\")\n",
        "    .when((col(\"Screen Time (Minutes)\") > 180) & (col(\"Screen Time (Minutes)\") <= 360), \"Moderate Usage\")\n",
        "    .otherwise(\"High Usage\")\n",
        ")\n",
        "agg_data = data.groupBy(\"Primary Use\").sum(\"Monthly Recharge Cost (INR)\").withColumnRenamed(\"sum(Monthly Recharge Cost (INR))\", \"Total Monthly Recharge Cost\")\n",
        "data.show()\n",
        "agg_data.show()\n",
        "data.write.csv(\"cleaned_phone_usage1.csv\", header=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "O8TNuqwm5X7I",
        "outputId": "69634fda-ba10-40cb-e3ad-c079eacaa09f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+------+---------+------------+-------+---------------------+---------------------+-------------------------+------------------------+---------------------------+----------------------------+------------------------+---------------------+---------------------------+-------------+------------+---------------------+---------------------------+------------------------+---------------------+\n",
            "|User ID|Age|Gender| Location| Phone Brand|     OS|Screen Time (hrs/day)|Data Usage (GB/month)|Calls Duration (mins/day)|Number of Apps Installed|Social Media Time (hrs/day)|E-commerce Spend (INR/month)|Streaming Time (hrs/day)|Gaming Time (hrs/day)|Monthly Recharge Cost (INR)|  Primary Use|Age_Category|Screen Time (Minutes)|Social Media Time (Minutes)|Streaming Time (Minutes)|Gaming Time (Minutes)|\n",
            "+-------+---+------+---------+------------+-------+---------------------+---------------------+-------------------------+------------------------+---------------------------+----------------------------+------------------------+---------------------+---------------------------+-------------+------------+---------------------+---------------------------+------------------------+---------------------+\n",
            "| U00001| 53|  Male|   Mumbai|        Vivo|Android|                  3.7|                 23.9|                     37.9|                     104|                        3.9|                         469|                     5.2|                  4.1|                        803|    Education| Middle-aged|       Moderate Usage|                      234.0|                   312.0|   245.99999999999997|\n",
            "| U00002| 60| Other|    Delhi|      Realme|  apple|                  9.2|                 28.1|                     13.7|                     169|                        2.8|                        4997|                     5.1|                  0.4|                       1526|       Gaming| Middle-aged|           High Usage|                      168.0|                   306.0|                 24.0|\n",
            "| U00003| 37|Female|Ahmedabad|       Nokia|Android|                  4.5|                 12.3|                     66.8|                      96|                        3.0|                        2381|                     1.7|                  2.9|                       1619|Entertainment| Middle-aged|       Moderate Usage|                      180.0|                   102.0|                174.0|\n",
            "| U00004| 32|  Male|     Pune|     Samsung|Android|                 11.0|                 25.6|                    156.2|                     146|                        5.2|                        1185|                     3.2|                  0.3|                       1560|Entertainment|       Young|           High Usage|                      312.0|                   192.0|                 18.0|\n",
            "| U00006| 21|  Male|   Jaipur|        Oppo|  apple|                  5.4|                 10.6|                    210.6|                      25|                        4.2|                        6285|                     0.6|                  4.8|                       1749|Entertainment|       Young|       Moderate Usage|                      252.0|                    36.0|                288.0|\n",
            "| U00008| 56| Other|  Kolkata|      Realme|  apple|                  3.1|                 43.5|                    125.3|                     188|                        2.3|                        9767|                     5.2|                  5.0|                       1136|Entertainment| Middle-aged|       Moderate Usage|                      138.0|                   312.0|                300.0|\n",
            "| U00009| 46|Female|  Kolkata|        Oppo|Android|                  5.3|                 46.4|                     21.3|                     194|                        3.7|                        2870|                     6.1|                  2.8|                       1253|Entertainment| Middle-aged|       Moderate Usage|                      222.0|                   366.0|                168.0|\n",
            "| U00012| 41|Female|    Delhi|        Oppo|Android|                  7.5|                 23.5|                     84.9|                     187|                        0.9|                         643|                     3.5|                  2.7|                        151| Social Media| Middle-aged|           High Usage|                       54.0|                   210.0|                162.0|\n",
            "| U00013| 53|  Male|  Unknown|      Realme|  apple|                 10.5|                  1.4|                     33.7|                     137|                        4.5|                        7854|                     7.2|                  1.4|                        240|    Education| Middle-aged|           High Usage|                      270.0|                   432.0|                 84.0|\n",
            "| U00015| 33|Female|  Unknown|     Samsung|  apple|                  1.7|                 33.2|                    208.8|                     121|                        3.0|                        3185|                     4.2|                  0.5|                       1046|Entertainment|       Young|            Low Usage|                      180.0|                   252.0|                 30.0|\n",
            "| U00017| 46| Other|  Unknown|       Apple|Android|                  3.1|                 23.6|                    153.9|                     183|                        2.5|                        6242|                     6.2|                  0.4|                       1679|    Education| Middle-aged|       Moderate Usage|                      150.0|                   372.0|                 24.0|\n",
            "| U00018| 54|  Male|  Kolkata|       Apple|Android|                 11.6|                 12.6|                    210.4|                      62|                        4.9|                        4642|                     0.9|                  4.1|                       1360|Entertainment| Middle-aged|           High Usage|                      294.0|                    54.0|   245.99999999999997|\n",
            "| U00019| 50|Female|  Chennai|Google Pixel|Android|                  5.3|                  5.2|                    235.6|                     188|                        4.6|                        1120|                     1.0|                  0.8|                       1639|         Work| Middle-aged|       Moderate Usage|                      276.0|                    60.0|                 48.0|\n",
            "| U00020| 40| Other|Hyderabad|    Motorola|  apple|                  9.0|                 38.9|                    289.6|                      62|                        2.4|                        8012|                     7.7|                  2.0|                       1224|         Work| Middle-aged|           High Usage|                      144.0|                   462.0|                120.0|\n",
            "| U00022| 59| Other|     Pune|      Xiaomi|Android|                 11.7|                  9.1|                    203.9|                     101|                        5.8|                        2579|                     2.6|                  4.1|                        446|       Gaming| Middle-aged|           High Usage|                      348.0|                   156.0|   245.99999999999997|\n",
            "| U00023| 46| Other|  Chennai|     Samsung|Android|                  8.1|                 15.5|                    206.8|                      77|                        5.9|                        1776|                     3.4|                  2.5|                        899| Social Media| Middle-aged|           High Usage|                      354.0|                   204.0|                150.0|\n",
            "| U00025| 29|Female|Bangalore|      Realme|Android|                  7.0|                 41.7|                     51.0|                     130|                        1.4|                        2604|                     3.5|                  1.6|                       1154|    Education|       Young|           High Usage|                       84.0|                   210.0|                 96.0|\n",
            "| U00026| 36|  Male|Hyderabad|        Vivo|  apple|                  5.6|                 47.4|                    155.4|                     123|                        5.9|                        1886|                     2.3|                  3.0|                        955|         Work| Middle-aged|       Moderate Usage|                      354.0|                   138.0|                180.0|\n",
            "| U00028| 41| Other|   Jaipur|      Realme|Android|                  8.1|                 44.1|                     81.8|                      25|                        1.9|                        5545|                     5.7|                  3.7|                       1488|    Education| Middle-aged|           High Usage|                      114.0|                   342.0|                222.0|\n",
            "| U00029| 51|Female|  Kolkata|        Vivo|  apple|                 10.7|                 45.1|                     94.0|                     127|                        3.5|                        2293|                     6.7|                  5.0|                        333|    Education| Middle-aged|           High Usage|                      210.0|                   402.0|                300.0|\n",
            "+-------+---+------+---------+------------+-------+---------------------+---------------------+-------------------------+------------------------+---------------------------+----------------------------+------------------------+---------------------+---------------------------+-------------+------------+---------------------+---------------------------+------------------------+---------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-------------+---------------------------+\n",
            "|  Primary Use|Total Monthly Recharge Cost|\n",
            "+-------------+---------------------------+\n",
            "|    Education|                    3478342|\n",
            "|       Gaming|                    3464171|\n",
            "|Entertainment|                    3302462|\n",
            "|         Work|                    3499693|\n",
            "| Social Media|                    3433779|\n",
            "+-------------+---------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# **Audible**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Y8U3VUJrbejZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "spark = SparkSession.builder.appName(\"AudibleDatasetCleaning\").getOrCreate()\n",
        "data = spark.read.csv(\"audible_uncleaned.csv\", header=True, inferSchema=True)\n",
        "data = data.withColumn(\"author\", when(col(\"author\").isNull(), \"Unknown\").otherwise(col(\"author\")))\n",
        "data = data.withColumn(\"narrator\", when(col(\"narrator\").isNull(), \"Unknown\").otherwise(col(\"narrator\")))\n",
        "data = data.dropna(subset=[\"stars\"])\n",
        "data = data.filter((col(\"price\") >= 100) & (col(\"price\") <= 2000))\n",
        "data = data.withColumn(\"author\", regexp_replace(col(\"author\"), \"Writtenby:\\\\s*\", \"\"))\n",
        "data = data.withColumn(\"narrator\", regexp_replace(col(\"narrator\"), \"Narratedby:\\\\s*\", \"\"))\n",
        "def convert_time_to_minutes(time_str):\n",
        "    if not time_str:\n",
        "        return None\n",
        "    parts = time_str.split(\" and \")\n",
        "    hours = int(parts[0].split(\" \")[0]) if \"hrs\" in parts[0] else 0\n",
        "    minutes = int(parts[1].split(\" \")[0]) if len(parts) > 1 else 0\n",
        "    return hours * 60 + minutes\n",
        "time_udf = udf(convert_time_to_minutes, IntegerType())\n",
        "data = data.withColumn(\"time\", time_udf(col(\"time\")))\n",
        "data = data.withColumn(\"releasedate\", to_date(col(\"releasedate\"), \"MM/dd/yyyy\"))\n",
        "data = data.withColumn(\"n_stars\", regexp_extract(col(\"stars\"), r\"(\\d+(\\.\\d+)?)\\s*out\\s*of\", 1).cast(\"float\"))\n",
        "data = data.withColumn(\"ratings\", regexp_extract(col(\"stars\"), r\"(\\d+)\\s*ratings\", 1).cast(\"int\"))\n",
        "data=data.drop(\"stars\")\n",
        "data = data.withColumn(\"price\", regexp_replace(col(\"price\"), \"[^0-9]\", \"\").cast(\"int\"))\n",
        "language_counts = data.groupBy(\"language\").agg(count(\"*\").alias(\"audiobook_count\"))\n",
        "data.write.csv(\"cleaned_audible_dataset342.csv\", header=True,mode=\"overwrite\")\n",
        "data.show()\n",
        "language_counts.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SWSTxRgmk-12",
        "outputId": "6a01b6da-7ede-4733-9e91-4281db0a2f5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------------+--------------+----+-----------+--------+-----+-------+-------+\n",
            "|                name|          author|      narrator|time|releasedate|language|price|n_stars|ratings|\n",
            "+--------------------+----------------+--------------+----+-----------+--------+-----+-------+-------+\n",
            "|Geronimo Stilton ...| GeronimoStilton|    BillLobely| 140|       NULL| English|  468|    5.0|     34|\n",
            "|    The Burning Maze|     RickRiordan| RobbieDaymond| 788|       NULL| English|  820|    4.5|     41|\n",
            "|        The Deep End|      JeffKinney|    DanRussell| 123|       NULL| English|  410|    4.5|     38|\n",
            "|Daughter of the Deep|     RickRiordan|SoneelaNankani| 676|       NULL| English|  615|    4.5|     12|\n",
            "|The Lightning Thi...|     RickRiordan|JesseBernstein| 600|       NULL| English|  820|    4.5|    181|\n",
            "|The Hunger Games:...|  SuzanneCollins|TatianaMaslany| 635|       NULL| English|  656|    5.0|     72|\n",
            "|Quest for the Dia...|    WinterMorgan|   LukeDaniels| 143|       NULL| English|  233|    5.0|     11|\n",
            "|   The Dark Prophecy|     RickRiordan| RobbieDaymond| 752|       NULL| English|  820|    5.0|     50|\n",
            "|   The Tyrantâ€™s Tomb|     RickRiordan| RobbieDaymond| 802|       NULL| English|  820|    5.0|     58|\n",
            "|The Titan's Curse...|     RickRiordan|JesseBernstein| 528|       NULL| English|  820|    4.5|    130|\n",
            "|Magnus Chase and ...|     RickRiordan| MichaelCrouch| 778|       NULL| English|  820|    5.0|     41|\n",
            "|Geronimo Stilton ...| GeronimoStilton|    BillLobley| 145|       NULL| English|  467|    4.5|     33|\n",
            "|               Exile|ShannonMessenger|  CaitlinKelly| 881|       NULL| English|  836|    5.0|     20|\n",
            "|   The Tower of Nero|     RickRiordan| RobbieDaymond| 732|       NULL| English|  615|    5.0|     79|\n",
            "|Eldest: The Inher...|         Unknown|  GerrardDoyle|1409|       NULL| English|  957|    4.5|     47|\n",
            "|        Artemis Fowl|      EoinColfer|  GerryO'Brien| 406|       NULL| English|  683|    5.0|     27|\n",
            "|Geronimo Stilton ...| GeronimoStilton|    BillLobley| 157|       NULL| English|  469|    4.5|     25|\n",
            "|Percy Jackson and...|     RickRiordan|JesseBernstein| 632|       NULL| English|  820|    5.0|    101|\n",
            "|     Winnie-the-Pooh|       A.A.Milne|   PeterDennis| 166|       NULL| English|  468|    5.0|     15|\n",
            "|Geronimo Stilton ...|         Unknown|    BillLobley| 152|       NULL| English|  469|    5.0|     18|\n",
            "+--------------------+----------------+--------------+----+-----------+--------+-----+-------+-------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+---------+---------------+\n",
            "| language|audiobook_count|\n",
            "+---------+---------------+\n",
            "|  russian|             11|\n",
            "|   hebrew|              1|\n",
            "|   danish|              4|\n",
            "|    dutch|              6|\n",
            "|   german|             35|\n",
            "|  spanish|             29|\n",
            "|   french|             48|\n",
            "|afrikaans|              1|\n",
            "|  English|            288|\n",
            "|  swedish|              8|\n",
            "|  italian|              7|\n",
            "|  catalan|              9|\n",
            "|    Hindi|              1|\n",
            "|  finnish|              1|\n",
            "|   polish|              2|\n",
            "+---------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# **Crime**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "O8PW9Q-zdrMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "spark = SparkSession.builder.appName(\"CrimeDatasetCleaning\").getOrCreate()\n",
        "\n",
        "data = spark.read.csv(\"crime_data.csv\", header=True, inferSchema=True)\n",
        "\n",
        "data = data.fillna({\"Vict Sex\": \"Unknown\", \"Vict Age\": 0})\n",
        "data = data.filter((col(\"Vict Age\") >= 0) & (col(\"Vict Age\") <= 120))\n",
        "data = data.withColumn(\"TIME OCC\", lpad(col(\"TIME OCC\").cast(StringType()), 4, \"0\"))\n",
        "data = data.withColumn(\"TIME OCC\", concat_ws(\":\", col(\"TIME OCC\").substr(1, 2), col(\"TIME OCC\").substr(3, 2)))\n",
        "data = data.withColumn(\"AREA NAME\", regexp_replace(col(\"AREA NAME\"), r\"[\\\\|/]+\", \"\"))\n",
        "data = data.withColumn(\"Date Rptd\", to_timestamp(col(\"Date Rptd\"), \"MM/dd/yyyy hh:mm:ss a\")).dropna()\n",
        "data = data.withColumn(\"Date Rptd\", date_format(col(\"Date Rptd\"), \"yyyy-MM-dd\"))\n",
        "data = data.withColumn(\"Crime Code\", regexp_replace(col(\"Crime Code\"), r\"[^\\d]\", \"\"))\n",
        "data = data.withColumn(\"Year\", year(col(\"Date Rptd\")))\n",
        "data = data.withColumn(\"Month\", month(col(\"Date Rptd\")))\n",
        "\n",
        "area_crime_counts = data.groupBy(\"AREA NAME\").agg(count(\"*\").alias(\"Total Crimes\"))\n",
        "\n",
        "data.write.csv(\"cleaned_crime_dataset111.csv\", header=True, mode=\"overwrite\")\n",
        "\n",
        "data.show()\n",
        "area_crime_counts.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fm0R9uehlCgl",
        "outputId": "f383c740-606a-41ca-a813-7ee9ee97ff8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------+--------------------+--------+-----------+--------+----------+--------------------+--------+--------+---------+-----------+------------+-------+---------+----+-----+\n",
            "|    DR_NO| Date Rptd|            DATE OCC|TIME OCC|  AREA NAME|Part 1-2|Crime Code|     Crime Code Desc|Vict Age|Vict Sex|Premis Cd|Premis Desc| Status Desc|    LAT|      LON|Year|Month|\n",
            "+---------+----------+--------------------+--------+-----------+--------+----------+--------------------+--------+--------+---------+-----------+------------+-------+---------+----+-----+\n",
            "|201308739|2020-03-27|03/27/2020 12:00:...|   12:10|     Newton|       1|       510|    VEHICLE - STOLEN|       0| Unknown|      101|     STREET| Invest Cont| 34.017|-118.2643|2020|    3|\n",
            "|201112065|2020-07-31|07/30/2020 12:00:...|   20:30|  Northeast|       1|       510|    VEHICLE - STOLEN|       0| Unknown|      101|     STREET|Adult Arrest|34.0953|-118.2974|2020|    7|\n",
            "|201215394|2020-06-26|06/25/2020 12:00:...|   22:00|77th Street|       1|       510|    VEHICLE - STOLEN|       0| Unknown|      101|     STREET| Invest Cont|33.9747|-118.2587|2020|    6|\n",
            "|200912561|2020-07-20|07/20/2020 12:00:...|   07:00|  Van Nuys |       1|       510|    VEHICLE - STOLEN|       0| Unknown|      101|     STREET| Invest Cont|34.1812|-118.4356|2020|    7|\n",
            "|200207955|2020-03-19|03/19/2020 12:00:...|   01:30|   Rampart |       1|       510|    VEHICLE - STOLEN|       0| Unknown|      101|     STREET| Invest Cont|34.0632|-118.2693|2020|    3|\n",
            "|200215463|2020-09-26|09/25/2020 12:00:...|   19:00|    Rampart|       1|       510|    VEHICLE - STOLEN|       0| Unknown|      101|     STREET| Invest Cont|34.0668|-118.2627|2020|    9|\n",
            "|201000767|2020-07-14|07/13/2020 12:00:...|   20:00|West Valley|       1|       330|BURGLARY FROM VEH...|      41|       M|      108|PARKING LOT|Adult Arrest|34.1774|-118.5387|2020|    7|\n",
            "|200410136|2020-06-24|06/24/2020 12:00:...|   07:00| Hollenbeck|       1|       510|    VEHICLE - STOLEN|       0| Unknown|      101|     STREET| Invest Cont|34.0809|-118.2132|2020|    6|\n",
            "|200814461|2020-09-24|09/24/2020 12:00:...|   17:10|    West LA|       1|       510|    VEHICLE - STOLEN|       0| Unknown|      101|     STREET| Invest Cont|34.0375|-118.3886|2020|    9|\n",
            "|201215562|2020-06-29|06/29/2020 12:00:...|   07:30|77th Street|       1|       510|    VEHICLE - STOLEN|       0| Unknown|      101|     STREET| Invest Cont|33.9655|-118.3046|2020|    6|\n",
            "|200210168|2020-05-14|    05-12-2020 00:00|   21:00|    Rampart|       1|       510|    VEHICLE - STOLEN|       0| Unknown|      101|     STREET| Invest Cont|34.0531|-118.2738|2020|    5|\n",
            "|200714301|2020-09-27|09/22/2020 12:00:...|   14:30|   Wilshire|       1|       510|    VEHICLE - STOLEN|       0| Unknown|      101|     STREET| Invest Cont|34.0853|-118.3499|2020|    9|\n",
            "|200906009|2020-02-14|02/13/2020 12:00:...|   22:30|   Van Nuys|       2|       806|           PANDERING|       0|       F|      101|     STREET| Adult Other|34.1958|-118.4662|2020|    2|\n",
            "|200618540|2020-11-27|11/27/2020 12:00:...|   09:50|  Hollywood|       1|       510|    VEHICLE - STOLEN|       0| Unknown|      101|     STREET| Invest Cont|34.0981|-118.3577|2020|   11|\n",
            "|200316510|2020-08-30|08/29/2020 12:00:...|   20:50|  Southwest|       1|       510|    VEHICLE - STOLEN|       0| Unknown|      101|     STREET| Adult Other|34.0278|-118.3378|2020|    8|\n",
            "|200808682|2020-04-25|04/21/2020 12:00:...|   14:00|    West LA|       1|       510|    VEHICLE - STOLEN|       0| Unknown|      101|     STREET| Invest Cont|34.0548|-118.3837|2020|    4|\n",
            "|201107590|2020-03-22|03/21/2020 12:00:...|   20:00|  Northeast|       1|       510|    VEHICLE - STOLEN|       0| Unknown|      101|     STREET| Invest Cont|34.1109|-118.2016|2020|    3|\n",
            "|200119680|2020-10-19|10/18/2020 12:00:...|   18:00|    Central|       1|       510|    VEHICLE - STOLEN|       0| Unknown|      108|PARKING LOT| Invest Cont|34.0366|-118.2325|2020|   10|\n",
            "|200410224|2020-06-27|06/27/2020 12:00:...|   04:00| Hollenbeck|       1|       510|    VEHICLE - STOLEN|       0| Unknown|      108|PARKING LOT| Invest Cont|34.0427|-118.2237|2020|    6|\n",
            "|200109800|2020-03-29|03/29/2020 12:00:...|   09:30|    Central|       1|       330|BURGLARY FROM VEH...|       0|       M|      103|      ALLEY| Invest Cont|34.0416| -118.255|2020|    3|\n",
            "+---------+----------+--------------------+--------+-----------+--------+----------+--------------------+--------+--------+---------+-----------+------------+-------+---------+----+-----+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-----------+------------+\n",
            "|  AREA NAME|Total Crimes|\n",
            "+-----------+------------+\n",
            "|  Hollywood|          57|\n",
            "|     Harbor|          40|\n",
            "|  Van Nuys |           1|\n",
            "|   Rampart |           1|\n",
            "|     Newton|          13|\n",
            "|   Van Nuys|          41|\n",
            "|West Valley|          38|\n",
            "|   Wilshire|          46|\n",
            "|    Central|          63|\n",
            "| Hollenbeck|          47|\n",
            "|  Southwest|          62|\n",
            "|    Rampart|          49|\n",
            "|    West LA|          40|\n",
            "|77th Street|          66|\n",
            "|  Northeast|          46|\n",
            "+-----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# **Movie**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jdSuYbE8lDC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "spark = SparkSession.builder.appName(\"MovieDatasetCleaning\").getOrCreate()\n",
        "data = spark.read.csv(\"movie_dataset_uncleaned.csv\" , header=True, inferSchema=True)\n",
        "mean_rating = data.select(mean(col(\"Rating\"))).collect()[0][0]\n",
        "data = data.withColumn(\"Rating\", when(col(\"Rating\").isNull(), mean_rating).otherwise(col(\"Rating\")))\n",
        "median_votes = data.approxQuantile(\"Votes\", [0.5], 0)[0]\n",
        "data = data.withColumn(\"Votes\", when(col(\"Votes\").isNull(), median_votes).otherwise(col(\"Votes\")))\n",
        "data = data.fillna({\"RunTime\": 0, \"Genre\": \"Unknown\", \"Gross\": \"$0.00M\"})\n",
        "data = data.filter((col(\"Rating\") >= 1) & (col(\"Rating\") <= 10))\n",
        "data = data.withColumn(\"Year\", regexp_extract(col(\"Year\"), \"(\\\\d{4})\", 1).cast(IntegerType()))\n",
        "data = data.withColumn(\"Start_Year\", split(col(\"Year\"), \"â€“\").getItem(0).cast(IntegerType()))\n",
        "data = data.withColumn(\"End_Year\", split(col(\"Year\"), \"â€“\").getItem(1).cast(IntegerType()))\n",
        "data = data.withColumn(\"Start_Year\", when(col(\"Start_Year\").isNull() & col(\"End_Year\").isNotNull(), col(\"End_Year\") - 5)\n",
        "                       .otherwise(col(\"Start_Year\")))\n",
        "data = data.withColumn(\"End_Year\", when(col(\"End_Year\").isNull() & col(\"Start_Year\").isNotNull(), col(\"Start_Year\") + 5)\n",
        "                       .otherwise(col(\"End_Year\")))\n",
        "data = data.filter(col(\"Start_Year\").isNotNull() & col(\"End_Year\").isNotNull())\n",
        "data = data.withColumn(\"Final_Genre\", split(col(\"Genre\"), \"\\\\$\").getItem(0))\n",
        "data = data.withColumn(\"Final_Genre\", when(col(\"Final_Genre\").isNull() | (col(\"Final_Genre\") == \"\"), \"unknown\")\n",
        "                       .otherwise(lower(trim(col(\"Final_Genre\")))))\n",
        "data.write.csv(\"cleaned_movie_dataset.csv\", header=True,mode=\"overwrite\")\n",
        "data.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DQ_Ojujtu641",
        "outputId": "11df33e9-e6e3-4f30-d4d9-2ec409920c7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----+--------------------+------+---------+-------+------+----------+--------+-----------+\n",
            "|              MOVIES|Year|               GENRE|Rating|    Votes|RunTime| Gross|Start_Year|End_Year|Final_Genre|\n",
            "+--------------------+----+--------------------+------+---------+-------+------+----------+--------+-----------+\n",
            "|       Blood Red Sky|2021|Action$ Horror$ T...|   6.1|  21062.0|    121|$0.00M|      2021|    2026|     action|\n",
            "|Masters of the Un...|2021|Animation$ Action...|   5.0|  17870.0|     25|$0.00M|      2021|    2026|  animation|\n",
            "|    The Walking Dead|2010|Drama$ Horror$ Th...|   8.2| 885805.0|     44|$0.00M|      2010|    2015|      drama|\n",
            "|      Rick and Morty|2013|Animation$ Advent...|   9.2| 414849.0|     23|$0.00M|      2013|    2018|  animation|\n",
            "|         Outer Banks|2020|Action$ Crime$ Drama|   7.6|  25858.0|     50|$0.00M|      2020|    2025|     action|\n",
            "|The Last Letter f...|2021|      Drama$ Romance|   6.8|   5283.0|    110|$0.00M|      2021|    2026|      drama|\n",
            "|              Dexter|2006|Crime$ Drama$ Mys...|   8.6| 665387.0|     53|$0.00M|      2006|    2011|      crime|\n",
            "|   Never Have I Ever|2020|              Comedy|   7.9|  34530.0|     30|$0.00M|      2020|    2025|     comedy|\n",
            "|        Virgin River|2019|             Unknown|   7.4|  27279.0|     44|$0.00M|      2019|    2024|    unknown|\n",
            "| Gunpowder Milkshake|2021|Action$ Adventure...|   6.0|  17989.0|    114|$0.00M|      2021|    2026|     action|\n",
            "|             Lucifer|2016|Crime$ Drama$ Fan...|   8.1| 264222.0|     42|$0.00M|      2016|    2021|      crime|\n",
            "|   Fear Street: 1994|2021|Drama$ Horror$ My...|   6.2|  50148.0|    107|$0.00M|      2021|    2026|      drama|\n",
            "|            Sex/Life|2021|Comedy$ Drama$ Ro...|   5.4|  12172.0|      0|$0.00M|      2021|    2026|     comedy|\n",
            "|American Horror S...|2011|Drama$ Horror$ Th...|   8.0| 286488.0|     60|$0.00M|      2011|    2016|      drama|\n",
            "|      Grey's Anatomy|2005|      Drama$ Romance|   7.5| 266258.0|     41|$0.00M|      2005|    2010|      drama|\n",
            "|        Breaking Bad|2008|Crime$ Drama$ Thr...|   9.4|1552311.0|     49|$0.00M|      2008|    2013|      crime|\n",
            "|     The Good Doctor|2017|               Drama|   8.1|  70871.0|     41|$0.00M|      2017|    2022|      drama|\n",
            "|            Atypical|2017|       Comedy$ Drama|   8.3|  71544.0|     30|$0.00M|      2017|    2022|     comedy|\n",
            "|     Stranger Things|2016|Drama$ Fantasy$ H...|   8.7| 885856.0|     51|$0.00M|      2016|    2021|      drama|\n",
            "|   Fear Street: 1978|2021|Drama$ Horror$ My...|   6.8|  36634.0|    109|$0.00M|      2021|    2026|      drama|\n",
            "+--------------------+----+--------------------+------+---------+-------+------+----------+--------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Employee**"
      ],
      "metadata": {
        "id": "b075QVkqw45x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Emp\").getOrCreate()\n",
        "\n",
        "emp = spark.read.csv(\"emp_data.csv\", header=True, inferSchema=True)\n",
        "\n",
        "missing_counts = emp.select([count(when(col(c).isNull() | isnan(col(c)), c)).alias(c) for c in emp.columns])\n",
        "\n",
        "emps = emp.fillna({\"LastName\": \"Unknown\"})\n",
        "emp = emps.dropna(subset=[\"EmpID\", \"StartDate\"])\n",
        "\n",
        "emp = emp.withColumn(\"Current Employee Rating\",\n",
        "                     when(col(\"Current Employee Rating\") < 1, 1)\n",
        "                     .when(col(\"Current Employee Rating\") > 5, 5)\n",
        "                     .otherwise(col(\"Current Employee Rating\")))\n",
        "\n",
        "emp.select(\"LocationCode\").distinct().show()\n",
        "\n",
        "df = emp.dropDuplicates()\n",
        "df1 = df.groupBy(\"DepartmentType\", \"Title\").count().orderBy(\"DepartmentType\", \"Title\")\n",
        "df1.show()\n",
        "\n",
        "emp = emp.withColumn(\"Performance Score\",\n",
        "                     when(col(\"Performance Score\") == \"Exceeds\", 3)\n",
        "                     .when(col(\"Performance Score\") == \"Fully Meets\", 2)\n",
        "                     .when(col(\"Performance Score\") == \"Needs Improvement\", 1)\n",
        "                     .otherwise(0))\n",
        "\n",
        "\n",
        "window_spec = Window.partitionBy(\"DepartmentType\").orderBy(desc(\"Performance Score\"))\n",
        "\n",
        "top_performers = emp.withColumn(\"rank\", rank().over(window_spec)).filter(col(\"rank\") == 1).drop(\"rank\")\n",
        "top_performers.show()\n",
        "\n",
        "df.write.csv(\"cleaned_employee_dataset00.csv\", header=True, mode=\"overwrite\")\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "e7aFxcKUw4I0",
        "outputId": "ce4a3c02-8884-4538-d054-e9e5b4195c57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+\n",
            "|LocationCode|\n",
            "+------------+\n",
            "|        2122|\n",
            "|       97413|\n",
            "|       80424|\n",
            "|        1460|\n",
            "|       75321|\n",
            "|       49449|\n",
            "|       56687|\n",
            "|       78046|\n",
            "|       44553|\n",
            "|        9454|\n",
            "|       57754|\n",
            "|        8779|\n",
            "|       29811|\n",
            "|       65321|\n",
            "|       39859|\n",
            "|       49914|\n",
            "|       74388|\n",
            "|       34011|\n",
            "|       60107|\n",
            "|       31207|\n",
            "+------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-----------------+--------------------+-----+\n",
            "|   DepartmentType|               Title|count|\n",
            "+-----------------+--------------------+-----+\n",
            "| Executive Office|    Network Engineer|   18|\n",
            "|            IT/IS|  Area Sales Manager|    7|\n",
            "|            IT/IS|         BI Director|    1|\n",
            "|            IT/IS|                 CIO|    2|\n",
            "|            IT/IS|        Data Analyst|   10|\n",
            "|            IT/IS|       Data Analyst |    1|\n",
            "|            IT/IS|      Data Architect|    2|\n",
            "|            IT/IS|Database Administ...|   15|\n",
            "|            IT/IS|Enterprise Architect|    5|\n",
            "|            IT/IS|          IT Support|   44|\n",
            "|            IT/IS|    Network Engineer|   22|\n",
            "|            IT/IS|Principal Data Ar...|    8|\n",
            "|            IT/IS|Software Engineer...|    1|\n",
            "|            IT/IS|      Sr. Accountant|    4|\n",
            "|            IT/IS|             Sr. DBA|    6|\n",
            "|            IT/IS|Sr. Network Engineer|   30|\n",
            "|Production       |    Network Engineer|    1|\n",
            "|Production       |Production Techni...|    1|\n",
            "|Production       |Production Techni...|  190|\n",
            "|Production       |Production Techni...|   61|\n",
            "+-----------------+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-----+---------+---------+---------+---------+--------------------+--------------------+------------+--------------------+------------+-------+--------------------------+---------------+-----------------+--------------------+----------+-----+----------------------+----------+------------+--------+-----------+-----------------+-----------------------+\n",
            "|EmpID|FirstName| LastName|StartDate| ExitDate|               Title|          Supervisor|BusinessUnit|      EmployeeStatus|EmployeeType|PayZone|EmployeeClassificationType|TerminationType|   DepartmentType|            Division|       DOB|State|JobFunctionDescription|GenderCode|LocationCode|RaceDesc|MaritalDesc|Performance Score|Current Employee Rating|\n",
            "+-----+---------+---------+---------+---------+--------------------+--------------------+------------+--------------------+------------+-------+--------------------------+---------------+-----------------+--------------------+----------+-----+----------------------+----------+------------+--------+-----------+-----------------+-----------------------+\n",
            "| 3914|Nathanial|    Booth|04-Feb-20|29-Jan-21|    Network Engineer|          Jorge Mayo|         SVG|              Active|    Contract| Zone C|                 Temporary|    Resignation| Executive Office|           Engineers|22-07-1958|   MA|       Project Manager|      Male|        9758|   Other|     Single|                3|                      3|\n",
            "| 3442|   Kaylah|     Moon|09-Jul-19|16-Jun-22|  Area Sales Manager|        Ashley Scott|         PYZ|              Active|   Full-Time| Zone A|                 Full-Time|     Retirement|            IT/IS|    Field Operations|24-11-1952|   MA|               Laborer|      Male|        2810|   Black|     Single|                3|                      2|\n",
            "| 3829|Jaqueline|  Pittman|27-Jan-19|26-Feb-21|Sr. Network Engineer|        Jesse Newman|         BPC|              Active|   Part-Time| Zone C|                 Full-Time|    Resignation|            IT/IS|Project Managemen...|12-03-1956|   MA|               Manager|      Male|       55408|   Other|    Married|                3|                      3|\n",
            "| 3887|  Emanuel|     Haas|22-Apr-22|15-May-23|Sr. Network Engineer|          Jose Moran|         NEL|    Leave of Absence|    Contract| Zone B|                 Temporary|     Retirement|            IT/IS|       General - Con|24-06-1954|   CT|               Flagger|      Male|       54502|   White|   Divorced|                3|                      3|\n",
            "| 3917|    Chris|  Vincent|19-Dec-19|31-Mar-23|    Network Engineer|       Ethan Allison|         PYZ|              Active|   Part-Time| Zone C|                 Full-Time|     Retirement|            IT/IS|       General - Con|11-04-1993|   MA|            Technician|    Female|       89635|   Black|    Widowed|                3|                      3|\n",
            "| 3923|Giancarlo|   Juarez|25-Dec-21|19-Jun-23|Sr. Network Engineer|     Jennifer Lester|         WBL|              Active|   Full-Time| Zone C|                 Part-Time|    Involuntary|            IT/IS|              Aerial|16-03-1955|   MA|               Foreman|    Female|       26918|Hispanic|     Single|                3|                      3|\n",
            "| 3964|   Marlee|  Stevens|07-Jun-21|25-Jan-23|      Sr. Accountant|       Wendy Jackson|         PYZ|        Future Start|   Part-Time| Zone A|                 Temporary|    Involuntary|            IT/IS|              Aerial|26-02-2001|   MA|               Lineman|      Male|       21007|   Black|     Single|                3|                      3|\n",
            "| 3969|    Lewis|    Nixon|17-Jan-21|     NULL|Database Administ...|     Derrick Johnson|          PL|              Active|    Contract| Zone A|                 Full-Time|            Unk|            IT/IS|              Aerial|18-06-1966|   MA|               Lineman|    Female|       61086|   Asian|   Divorced|                3|                      3|\n",
            "| 3970|     Macy|  Webster|04-Oct-20|27-Apr-23|Database Administ...|     Cynthia Gardner|         PYZ|              Active|   Part-Time| Zone B|                 Full-Time|      Voluntary|            IT/IS|    Field Operations|08-11-1950|   MA|               Foreman|    Female|       44077|   Other|    Married|                3|                      3|\n",
            "| 3973| Brooklyn|   Tanner|21-Sep-18|22-Jun-23|Database Administ...|          Jason Sims|         WBL|Terminated for Cause|   Part-Time| Zone C|                 Temporary|    Resignation|            IT/IS|    Field Operations|22-05-1973|   TX|               Laborer|    Female|       50856|Hispanic|     Single|                3|                      3|\n",
            "| 3974|   Zander|   Franco|25-Dec-18|10-Jul-19|      Data Architect|       Ronnie Garcia|         BPC|Terminated for Cause|   Part-Time| Zone C|                 Full-Time|    Involuntary|            IT/IS|       General - Eng|07-01-1986|   TX|              Engineer|      Male|       97255|   Other|    Married|                3|                      3|\n",
            "| 3466|  Clayton|   Walker|13-Apr-22|10-Apr-23|  Area Sales Manager|          Jon Holden|         PYZ|              Active|   Part-Time| Zone C|                 Full-Time|     Retirement|            IT/IS|              Aerial|26-01-1979|   KY|            Supervisor|      Male|       64288|   Other|    Married|                3|                      4|\n",
            "| 3613|   Amirah|  Johnson|24-Feb-19|     NULL|Production Techni...|         Jason Owens|          PL|              Active|    Contract| Zone B|                 Part-Time|            Unk|Production       |    Field Operations|14-06-1973|   MA|               Laborer|      Male|        9241|   Asian|    Married|                3|                      1|\n",
            "| 3659|   Kaylee|  Baldwin|02-Oct-19|13-Oct-19|Production Techni...|         Eric Powell|        CCDR|              Active|   Full-Time| Zone A|                 Part-Time|      Voluntary|Production       |    Field Operations|30-10-1989|   MA|              Engineer|      Male|       86746|   Other|   Divorced|                3|                      1|\n",
            "| 3686|    Carly| Figueroa|25-Apr-21|12-Dec-22|Production Techni...|        Jesse Farmer|         BPC|        Future Start|   Part-Time| Zone A|                 Full-Time|     Retirement|Production       |           Engineers|20-04-1998|   MA|              Director|      Male|       67909|   White|     Single|                3|                      1|\n",
            "| 3687|     Bria|Mcpherson|17-May-19|     NULL|Production Techni...|       Brianna Blake|        CCDR|              Active|   Part-Time| Zone B|                 Temporary|            Unk|Production       |    Field Operations|20-08-1944|   MA|            Supervisor|    Female|       26340|   White|   Divorced|                3|                      1|\n",
            "| 3710|     Dean|Middleton|29-Oct-21|     NULL|Production Techni...|   Rebecca Henderson|          PL|              Active|   Full-Time| Zone C|                 Part-Time|            Unk|Production       |              Aerial|16-10-1960|   MA|               Foreman|    Female|       64630|Hispanic|     Single|                3|                      1|\n",
            "| 3562|     Jean|Crimmings|21-May-19|05-Jan-23|Production Techni...|   Stephen Rodriguez|        CCDR|        Future Start|    Contract| Zone B|                 Full-Time|     Retirement|Production       |        Shop (Fleet)|09-04-1954|   MA|               Manager|    Female|        1821|   Other|     Single|                3|                      2|\n",
            "| 3578|  Clinton|     Owad|04-May-19|     NULL|Production Techni...|Elizabeth Sulliva...|         WBL|              Active|   Full-Time| Zone A|                 Temporary|            Unk|Production       |    Field Operations|30-05-1957|   MA|            Technician|      Male|        1760|Hispanic|    Married|                3|                      2|\n",
            "| 3615|   Martin|    Weber|13-May-21|22-Sep-22|Production Techni...|          Lee Hudson|        CCDR|        Future Start|   Full-Time| Zone B|                 Part-Time|     Retirement|Production       |       General - Con|14-05-1986|   MA|               Manager|      Male|       74388|   Asian|   Divorced|                3|                      2|\n",
            "+-----+---------+---------+---------+---------+--------------------+--------------------+------------+--------------------+------------+-------+--------------------------+---------------+-----------------+--------------------+----------+-----+----------------------+----------+------------+--------+-----------+-----------------+-----------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-----+---------+--------------+---------+---------+--------------------+--------------------+------------+----------------+------------+-------+--------------------------+---------------+-----------------+--------------------+----------+-----+----------------------+----------+------------+--------+-----------+-----------------+-----------------------+\n",
            "|EmpID|FirstName|      LastName|StartDate| ExitDate|               Title|          Supervisor|BusinessUnit|  EmployeeStatus|EmployeeType|PayZone|EmployeeClassificationType|TerminationType|   DepartmentType|            Division|       DOB|State|JobFunctionDescription|GenderCode|LocationCode|RaceDesc|MaritalDesc|Performance Score|Current Employee Rating|\n",
            "+-----+---------+--------------+---------+---------+--------------------+--------------------+------------+----------------+------------+-------+--------------------------+---------------+-----------------+--------------------+----------+-----+----------------------+----------+------------+--------+-----------+-----------------+-----------------------+\n",
            "| 3461|  Lincoln|       Compton|18-Jul-19|01-Oct-21|  Area Sales Manager|        Tammy Conner|          EW|    Future Start|   Full-Time| Zone C|                 Part-Time|    Resignation|            Sales|       General - Con|29-12-1997|   CO|               Foreman|      Male|       78046|Hispanic|    Married|Needs Improvement|                      1|\n",
            "| 3736|   Carsen|       Wheeler|18-Oct-20|22-Mar-23|Production Techni...|       Kimberly Soto|         SVG|          Active|   Full-Time| Zone A|                 Part-Time|    Resignation|Production       |Wireline Construc...|02-07-1976|   MA|               Laborer|      Male|       95345|   Other|    Married|      Fully Meets|                      2|\n",
            "| 3928|  Audrina|            Yu|27-Jan-21|18-Oct-21|    Network Engineer|       Walter Powell|         TNS|          Active|   Full-Time| Zone A|                 Temporary|    Resignation|            IT/IS|       General - Con|11-07-1956|   MA|               Laborer|      Male|       59834|   White|    Widowed|      Fully Meets|                      3|\n",
            "| 3574|     Anna|Von Massenbach|18-May-21|04-Jan-22|Production Techni...|      Timothy Weaver|         WBL|    Future Start|    Contract| Zone B|                 Full-Time|    Involuntary|Production       |         Underground|06-05-1951|   MA|               Laborer|    Female|        2124|Hispanic|    Widowed|      Fully Meets|                      1|\n",
            "| 3883|    Noemi|        Brewer|06-May-22|26-Sep-22|          IT Support|      Jacob Hamilton|         BPC|          Active|    Contract| Zone B|                 Full-Time|    Involuntary|            IT/IS|    Field Operations|16-06-1991|   MA|         Administrator|    Female|       11294|   White|    Widowed|      Fully Meets|                      3|\n",
            "| 3847|  Gilbert|         Klein|24-Feb-19|11-Jun-20|          IT Support|   Gregory Rodriguez|          PL|          Active|   Part-Time| Zone B|                 Temporary|     Retirement|            IT/IS|    Field Operations|06-09-1998|   MA|               Laborer|      Male|       53800|Hispanic|     Single|      Fully Meets|                      3|\n",
            "| 3879|     Liam|         Hanna|08-Feb-19|24-Feb-23|    Network Engineer|       Brian Aguirre|         PYZ|          Active|   Full-Time| Zone C|                 Part-Time|    Involuntary|            IT/IS|    Field Operations|13-01-1996|   MA|                 Clerk|      Male|       16655|Hispanic|    Married|      Fully Meets|                      3|\n",
            "| 3893|    Lukas|       Freeman|22-Mar-23|28-Jul-23|          IT Support|     Allison Shannon|          EW|          Active|   Full-Time| Zone B|                 Part-Time|    Involuntary|            IT/IS|       General - Con|15-04-1953|   MA|               Laborer|    Female|       68298|   Asian|    Married|      Fully Meets|                      3|\n",
            "| 3820|    Anita|       Shepard|11-Aug-18|21-May-21|    Network Engineer|         George Tran|          EW|          Active|    Contract| Zone C|                 Full-Time|      Voluntary|            IT/IS|       General - Con|29-09-1976|   MA|               Foreman|    Female|        1773|   Other|   Divorced|      Fully Meets|                      3|\n",
            "| 3592|    Andre|        Guzman|05-Oct-21|18-Jun-22|Production Techni...|Mrs. Audrey Gonza...|         TNS|    Future Start|    Contract| Zone B|                 Full-Time|      Voluntary|Production       |    Field Operations|23-12-1998|   MA|            Technician|      Male|        9454|   Other|     Single|      Fully Meets|                      3|\n",
            "| 3703|  Addisyn|      Guerrero|31-Aug-20|26-Jun-22|Production Techni...|            Cory Lee|          EW|          Active|   Part-Time| Zone A|                 Part-Time|    Resignation|Production       |           Engineers|15-02-1951|   MA|              Engineer|    Female|       87563|   White|    Widowed|          Exceeds|                      4|\n",
            "| 3525|  Clayton|     Mccormick|03-Jul-22|10-Jan-23|  Area Sales Manager|      Stephanie Duke|         MSC|    Future Start|    Contract| Zone B|                 Temporary|     Retirement|            Sales|       General - Con|29-01-1967|   TX|               Foreman|      Male|       72264|Hispanic|     Single|      Fully Meets|                      2|\n",
            "| 3814|    Jyoti|        Lajiri|07-Jan-19|05-Sep-20|Sr. Network Engineer|      Karen Crawford|          EW|Leave of Absence|   Part-Time| Zone C|                 Full-Time|     Retirement|            IT/IS|           Engineers|17-04-1955|   MA|           Coordinator|      Male|        2169|   Black|    Widowed|      Fully Meets|                      3|\n",
            "| 3822|  Katrina|       Lambert|24-Sep-19|04-Apr-22|    Network Engineer|   Calvin Williamson|         WBL|          Active|   Full-Time| Zone B|                 Part-Time|     Retirement| Executive Office|       General - Con|18-09-1975|   MA|            Technician|      Male|       56727|Hispanic|   Divorced|      Fully Meets|                      3|\n",
            "| 3766|    Elisa|         Henry|02-Feb-19|03-Feb-23|Production Techni...|       Taylor Morris|         PYZ|          Active|   Part-Time| Zone A|                 Part-Time|    Resignation|Production       |Yard (Material Ha...|10-03-1943|   MA|               Manager|    Female|       10110|   Black|    Widowed|      Fully Meets|                      3|\n",
            "| 3807|    Mayra|          Moss|15-Sep-21|10-Nov-21|Production Techni...|        George Doyle|          EW|          Active|    Contract| Zone C|                 Temporary|    Resignation|Production       |Project Managemen...|16-05-1964|   MA|               Manager|      Male|       84074|   Asian|     Single|      Fully Meets|                      3|\n",
            "| 3512|   Tyrone|          Sosa|17-Apr-23|22-Jun-23|  Area Sales Manager|       Lindsay Chang|         NEL|    Future Start|    Contract| Zone C|                 Temporary|      Voluntary|            Sales|Yard (Material Ha...|25-10-1996|   TX|           Coordinator|    Female|       58860|   Black|     Single|      Fully Meets|                      2|\n",
            "| 3541|    Aaron|         Weber|06-Apr-20|24-Jul-23|  Area Sales Manager|      Melanie Garcia|         SVG|          Active|   Full-Time| Zone B|                 Full-Time|    Involuntary|            Sales|    Field Operations|27-10-1991|   IN|            Technician|      Male|       78938|   Black|    Married|          Exceeds|                      2|\n",
            "| 3675|    Mario|          Mays|08-Jun-20|29-Dec-22|Production Techni...|      Melissa Torres|        CCDR|          Active|   Full-Time| Zone C|                 Full-Time|    Resignation|Production       |           Engineers|28-08-1952|   MA|            Technician|    Female|       70090|Hispanic|   Divorced|      Fully Meets|                      3|\n",
            "| 3518|  Roberto|       Michael|10-Apr-21|04-Jul-22|  Area Sales Manager|        Cheryl Henry|         MSC|          Active|    Contract| Zone B|                 Part-Time|    Involuntary|            Sales|       General - Eng|24-05-1969|   CA|           Coordinator|    Female|       13249|Hispanic|    Married|      Fully Meets|                      2|\n",
            "+-----+---------+--------------+---------+---------+--------------------+--------------------+------------+----------------+------------+-------+--------------------------+---------------+-----------------+--------------------+----------+-----+----------------------+----------+------------+--------+-----------+-----------------+-----------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Sales**"
      ],
      "metadata": {
        "id": "ZkaZIW5Ow4xd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "spark = SparkSession.builder.appName(\"DataCleaning\").getOrCreate()\n",
        "df = spark.read.csv(\"Sales Data.csv\", header=True, inferSchema=True)\n",
        "numerical_columns = ['Sales', 'Quantity Ordered']\n",
        "for col_name in numerical_columns:\n",
        "    mean_value = df.select(mean(col_name)).collect()[0][0]\n",
        "    df = df.fillna({col_name: mean_value})\n",
        "df = df.dropna()\n",
        "df = df.dropDuplicates()\n",
        "df = df.withColumn(\"Sales\", col(\"Sales\").cast(\"float\"))\n",
        "df = df.withColumn(\"Quantity Ordered\", col(\"Quantity Ordered\").cast(\"integer\"))\n",
        "df = df.withColumn(\"Price Each\", col(\"Price Each\").cast(\"float\"))\n",
        "df.printSchema()\n",
        "columns_to_check = ['Sales', 'Price Each', 'Quantity Ordered']\n",
        "for col_name in columns_to_check:\n",
        "    df = df.filter(col(col_name) >= 0)\n",
        "total_sales=df.groupBy(\"Product\").sum(\"Sales\").withColumnRenamed(\"sum(Sales)\", \"Total Sales\")\n",
        "total_sales.show()\n",
        "df.write.csv(\"cleaned_sales_dataset12.csv\", header=True,mode=\"overwrite\")\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "b8_XBSFbw3_w",
        "outputId": "74619ecf-f6c4-428c-b02a-de582228762e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _c0: integer (nullable = true)\n",
            " |-- Order ID: integer (nullable = true)\n",
            " |-- Product: string (nullable = true)\n",
            " |-- Quantity Ordered: integer (nullable = true)\n",
            " |-- Price Each: float (nullable = true)\n",
            " |-- Order Date: string (nullable = true)\n",
            " |-- Purchase Address: string (nullable = true)\n",
            " |-- Month: integer (nullable = true)\n",
            " |-- Sales: float (nullable = false)\n",
            " |-- City: string (nullable = true)\n",
            " |-- Hour: integer (nullable = true)\n",
            "\n",
            "+--------------------+------------------+\n",
            "|             Product|       Total Sales|\n",
            "+--------------------+------------------+\n",
            "|    Wired Headphones|246651.92929840088|\n",
            "|  Macbook Pro Laptop|         8037600.0|\n",
            "|Apple Airpods Hea...|         2349150.0|\n",
            "|              iPhone|         4794300.0|\n",
            "|Lightning Chargin...|347094.14547920227|\n",
            "|Bose SoundSport H...|1345565.4012680054|\n",
            "|USB-C Charging Cable| 286674.7890357971|\n",
            "|AAA Batteries (4-...| 92740.83064889908|\n",
            "|        20in Monitor| 454148.7011795044|\n",
            "|    27in FHD Monitor|1132424.5414733887|\n",
            "|     Vareebadd Phone|          827200.0|\n",
            "|34in Ultrawide Mo...|2355557.9494628906|\n",
            "|            LG Dryer|          387600.0|\n",
            "|AA Batteries (4-p...|106300.05223083496|\n",
            "|        Google Phone|         3319200.0|\n",
            "|       Flatscreen TV|         1445700.0|\n",
            "|  LG Washing Machine|          399600.0|\n",
            "|27in 4K Gaming Mo...|2435097.4990234375|\n",
            "|     ThinkPad Laptop|4129958.6596679688|\n",
            "+--------------------+------------------+\n",
            "\n",
            "+----+--------+--------------------+----------------+----------+----------------+--------------------+-----+------+--------------+----+\n",
            "| _c0|Order ID|             Product|Quantity Ordered|Price Each|      Order Date|    Purchase Address|Month| Sales|          City|Hour|\n",
            "+----+--------+--------------------+----------------+----------+----------------+--------------------+-----+------+--------------+----+\n",
            "| 297|  295941|     ThinkPad Laptop|               1|    999.99|31-12-2019 16:24|64 Dogwood St, Po...|   12|999.99|      Portland|  16|\n",
            "| 464|  296105|Lightning Chargin...|               1|     14.95|29-12-2019 17:57|134 Dogwood St, S...|   12| 14.95| San Francisco|  17|\n",
            "| 532|  296169|              iPhone|               1|     700.0|10-12-2019 22:31|111 Hickory St, S...|   12| 700.0|       Seattle|  22|\n",
            "| 628|  296263|    Wired Headphones|               1|     11.99|11-12-2019 18:04|229 Pine St, San ...|   12| 11.99| San Francisco|  18|\n",
            "| 721|  296351|     ThinkPad Laptop|               1|    999.99|24-12-2019 17:57|168 10th St, Port...|   12|999.99|      Portland|  17|\n",
            "|1203|  296811|34in Ultrawide Mo...|               1|    379.99|10-12-2019 07:39|485 11th St, Port...|   12|379.99|      Portland|   7|\n",
            "|1395|  296992|Apple Airpods Hea...|               1|     150.0|09-12-2019 18:46|410 Washington St...|   12| 150.0|        Dallas|  18|\n",
            "|1414|  297009|  Macbook Pro Laptop|               1|    1700.0|16-12-2019 08:07|779 Maple St, San...|   12|1700.0| San Francisco|   8|\n",
            "|1431|  297025|34in Ultrawide Mo...|               1|    379.99|18-12-2019 12:04|836 Forest St, Bo...|   12|379.99|        Boston|  12|\n",
            "|1670|  297251|              iPhone|               1|     700.0|19-12-2019 20:19|835 Main St, Aust...|   12| 700.0|        Austin|  20|\n",
            "|1782|  297356|27in 4K Gaming Mo...|               1|    389.99|18-12-2019 22:01|769 14th St, Dall...|   12|389.99|        Dallas|  22|\n",
            "|1982|  297548|USB-C Charging Cable|               1|     11.95|27-12-2019 23:04|24 Pine St, New Y...|   12| 11.95| New York City|  23|\n",
            "|2037|  297596|       Flatscreen TV|               1|     300.0|10-12-2019 18:45|301 13th St, Los ...|   12| 300.0|   Los Angeles|  18|\n",
            "|2563|  298104|AA Batteries (4-p...|               1|      3.84|16-12-2019 22:30|955 Johnson St, L...|   12|  3.84|   Los Angeles|  22|\n",
            "|2746|  298285|Lightning Chargin...|               1|     14.95|21-12-2019 12:19|966 Lincoln St, S...|   12| 14.95| San Francisco|  12|\n",
            "|2887|  298422|Lightning Chargin...|               1|     14.95|19-12-2019 20:24|249 Wilson St, Da...|   12| 14.95|        Dallas|  20|\n",
            "|3431|  298934|    Wired Headphones|               1|     11.99|11-12-2019 10:28|400 Highland St, ...|   12| 11.99|       Seattle|  10|\n",
            "|3469|  298972|    Wired Headphones|               1|     11.99|06-12-2019 14:32|80 Elm St, Los An...|   12| 11.99|   Los Angeles|  14|\n",
            "|4135|  299597|Bose SoundSport H...|               1|     99.99|05-12-2019 10:29|606 Willow St, Ne...|   12| 99.99| New York City|  10|\n",
            "|4244|  299700|AA Batteries (4-p...|               1|      3.84|20-12-2019 06:34|371 Cedar St, Dal...|   12|  3.84|        Dallas|   6|\n",
            "+----+--------+--------------------+----------------+----------+----------------+--------------------+-----+------+--------------+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Job**"
      ],
      "metadata": {
        "id": "8ng2L7otw4o8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.appName(\"jobs\").getOrCreate()\n",
        "\n",
        "jobs = spark.read.csv(\"Jobs.csv\", header=True, inferSchema=True)\n",
        "\n",
        "jobs = jobs.withColumn(\"Salary Estimate\", regexp_replace(col(\"Salary Estimate\"), r\"\\s+\", \"\"))\n",
        "\n",
        "jobs = jobs.withColumn(\"min_salary\", regexp_extract(col(\"Salary Estimate\"), r\"(\\d+)-\", 1).cast(\"double\"))\n",
        "jobs = jobs.withColumn(\"max_salary\", regexp_extract(col(\"Salary Estimate\"), r\"-(\\d+)\", 1).cast(\"double\"))\n",
        "\n",
        "jobs = jobs.withColumn(\"avg_sal\", (col(\"min_salary\") + col(\"max_salary\")) / 2)\n",
        "\n",
        "jobs = jobs.withColumn(\"Rating\", when((col(\"Rating\") == 0) | (col(\"Rating\") == -1), 1).otherwise(col(\"Rating\")))\n",
        "\n",
        "columns_with_nulls = [col_name for col_name in jobs.columns if jobs.filter(col(col_name).isNull()).count() > 0]\n",
        "df_cleaned = jobs.fillna({col_name: -1 for col_name in columns_with_nulls})\n",
        "\n",
        "df_cleaned = df_cleaned.withColumn(\"company_size_category\",\n",
        "    when(col(\"Size\").contains(\"1 to 50\") | col(\"Size\").contains(\"51 to 200\"), \"Small\")\n",
        "    .when(col(\"Size\").contains(\"201 to 500\") | col(\"Size\").contains(\"501 to 1000\"), \"Medium\")\n",
        "    .when(col(\"Size\").contains(\"1001 to 5000\") | col(\"Size\").contains(\"5001 to 10000\") |\n",
        "          col(\"Size\").contains(\"10000+\"), \"Large\")\n",
        "    .otherwise(\"Unknown\"))\n",
        "\n",
        "salary_by_size = df_cleaned.groupBy(\"company_size_category\").agg(avg(\"avg_sal\").alias(\"average_salary\")).orderBy(\"company_size_category\")\n",
        "\n",
        "salary_by_size.show()\n",
        "df_cleaned.show()\n",
        "df_cleaned.write.csv(\"jobies\", header=True, mode=\"overwrite\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fjztdF6gw3Sx",
        "outputId": "d155148d-d879-4eb5-9dd8-2fd83c0257e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------+------------------+\n",
            "|company_size_category|    average_salary|\n",
            "+---------------------+------------------+\n",
            "|                Large|124.48571428571428|\n",
            "|               Medium|120.93506493506493|\n",
            "|                Small|122.80325814536342|\n",
            "|              Unknown| 135.6590909090909|\n",
            "+---------------------+------------------+\n",
            "\n",
            "+--------------------+---------------+------+-----------------+--------------------+--------------------+--------------------+--------------------+---------+-----------+------+-----+-------+----------+----------+-------+---------------------+\n",
            "|           Job Title|Salary Estimate|Rating|         Location|                Size|   Type of ownership|            Industry|              Sector|job_state|company_age|python|spark|tableau|min_salary|max_salary|avg_sal|company_size_category|\n",
            "+--------------------+---------------+------+-----------------+--------------------+--------------------+--------------------+--------------------+---------+-----------+------+-----+-------+----------+----------+-------+---------------------+\n",
            "|   Sr Data Scientist|        137-171|   3.1|     New York, NY|1001 to 5000 empl...|Nonprofit Organiz...|  Insurance Carriers|           Insurance|       NY|         27|     0|    0|      0|     137.0|     171.0|  154.0|                Small|\n",
            "|      Data Scientist|        137-171|   4.2|    Chantilly, VA|5001 to 10000 emp...|    Company - Public|Research & Develo...|   Business Services|       VA|         52|     0|    0|      0|     137.0|     171.0|  154.0|                Large|\n",
            "|      Data Scientist|        137-171|   3.8|       Boston, MA|1001 to 5000 empl...|Private Practice ...|          Consulting|   Business Services|       MA|         39|     1|    0|      0|     137.0|     171.0|  154.0|                Small|\n",
            "|      Data Scientist|        137-171|   3.5|       Newton, MA|501 to 1000 emplo...|    Company - Public|Electrical & Elec...|       Manufacturing|       MA|         20|     1|    0|      0|     137.0|     171.0|  154.0|               Medium|\n",
            "|      Data Scientist|        137-171|   2.9|     New York, NY| 51 to 200 employees|   Company - Private|Advertising & Mar...|   Business Services|       NY|         22|     1|    0|      0|     137.0|     171.0|  154.0|                Small|\n",
            "|      Data Scientist|        137-171|   4.2|Santa Barbara, CA| 51 to 200 employees|   Company - Private|Computer Hardware...|Information Techn...|       CA|         10|     1|    1|      0|     137.0|     171.0|  154.0|                Small|\n",
            "|Data Scientist / ...|        137-171|   3.9|    Cambridge, MA|    10000+ employees|    Company - Public|Biotech & Pharmac...|Biotech & Pharmac...|       MA|         24|     1|    0|      0|     137.0|     171.0|  154.0|                Large|\n",
            "|      Data Scientist|        137-171|   3.5|      Bedford, MA|1001 to 5000 empl...|    Company - Public|Consumer Electron...|              Retail|       MA|         30|     1|    0|      0|     137.0|     171.0|  154.0|                Small|\n",
            "|Staff Data Scient...|        137-171|   4.4|    San Diego, CA|5001 to 10000 emp...|    Company - Public|Computer Hardware...|Information Techn...|       CA|         37|     0|    0|      0|     137.0|     171.0|  154.0|                Large|\n",
            "|      Data Scientist|        137-171|   3.6|      Chicago, IL| 51 to 200 employees|   Company - Private|Enterprise Softwa...|Information Techn...|       IL|          6|     1|    0|      0|     137.0|     171.0|  154.0|                Small|\n",
            "|      Data Scientist|        137-171|   4.5|      Herndon, VA|501 to 1000 emplo...|   Company - Private|Enterprise Softwa...|Information Techn...|       VA|          8|     1|    0|      0|     137.0|     171.0|  154.0|               Medium|\n",
            "|      Data Scientist|        137-171|   4.7|  Saint Louis, MO| 51 to 200 employees|   Company - Private|         IT Services|Information Techn...|       MO|          4|     1|    1|      0|     137.0|     171.0|  154.0|                Small|\n",
            "|Data Scientist - ...|        137-171|   3.7|     Richland, WA|1001 to 5000 empl...|          Government|              Energy|Oil, Gas, Energy ...|       WA|         55|     0|    0|      0|     137.0|     171.0|  154.0|                Small|\n",
            "|        Data Modeler|        137-171|   3.1|   Northbrook, IL|201 to 500 employees|   Company - Private|Chemical Manufact...|       Manufacturing|       IL|         47|     1|    0|      1|     137.0|     171.0|  154.0|                Small|\n",
            "|      Data Scientist|        137-171|   3.4|   Washington, DC|1001 to 5000 empl...|   Company - Private|          Consulting|   Business Services|       DC|         34|     1|    0|      0|     137.0|     171.0|  154.0|                Small|\n",
            "|Experienced Data ...|        137-171|   4.4|   Washington, DC|   1 to 50 employees|   Company - Private|    Federal Agencies|          Government|       DC|         23|     1|    1|      1|     137.0|     171.0|  154.0|                Small|\n",
            "|      Data Scientist|        137-171|   3.5|      Memphis, TN|1001 to 5000 empl...|   Company - Private|Chemical Manufact...|       Manufacturing|       TN|         75|     1|    0|      0|     137.0|     171.0|  154.0|                Small|\n",
            "|     Data Analyst II|        137-171|   4.2|        Plano, TX|5001 to 10000 emp...|    Company - Public|Enterprise Softwa...|Information Techn...|       TX|         32|     0|    0|      0|     137.0|     171.0|  154.0|                Large|\n",
            "|Medical Lab Scien...|        137-171|   3.5|   West Grove, PA|5001 to 10000 emp...|Nonprofit Organiz...|Health Care Servi...|         Health Care|       PA|          3|     0|    0|      0|     137.0|     171.0|  154.0|                Large|\n",
            "|      Data Scientist|        137-171|   3.2|     New York, NY| 51 to 200 employees|   Company - Private|Computer Hardware...|Information Techn...|       NY|          5|     0|    0|      0|     137.0|     171.0|  154.0|                Small|\n",
            "+--------------------+---------------+------+-----------------+--------------------+--------------------+--------------------+--------------------+---------+-----------+------+-----+-------+----------+----------+-------+---------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}